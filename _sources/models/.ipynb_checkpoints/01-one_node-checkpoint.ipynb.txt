{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# ICM with One Node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "一个节点的信息因果模型 with a loop 是最简单的。We consider an Info Causal Model(ICM) with the causal graph $G=(V, E)$ where $V = \\{X\\}$ and $E=\\{[X, X]\\}$. 我们本文的目的是希望用这个最简单的模型阐述清楚 ICM 的构建，训练，和推断(causal and statistical inference)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"80pt\" height=\"44pt\"\n",
       " viewBox=\"0.00 0.00 80.00 44.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 40)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-40 76,-40 76,4 -4,4\"/>\n",
       "<!-- X -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>X</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\">X</text>\n",
       "</g>\n",
       "<!-- X&#45;&gt;X -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>X&#45;&gt;X</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.44,-24.69C63.03,-25.15 72,-22.92 72,-18 72,-14.77 68.14,-12.7 62.49,-11.79\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.6,-8.29 52.44,-11.31 62.27,-15.28 62.6,-8.29\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x1115f0550>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Source\n",
    "Source('digraph{X->X}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "作为一个简单的 feedback system, 表面上看很简单，但是如果我们允许 $X$ 是一个复杂内部结构的张量，那么任何反馈系统都可以抽样成这种形式。因此我们需要研究在此简单模型上的因果推断。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "\n",
    "# for CI testing\n",
    "smoke_test = ('CI' in os.environ)\n",
    "assert pyro.__version__.startswith('1.3.0')\n",
    "pyro.enable_validation(True)\n",
    "pyro.set_rng_seed(1)\n",
    "pyro.enable_validation(True)\n",
    "\n",
    "\n",
    "# Set matplotlib settings\n",
    "%matplotlib inline\n",
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "## Data\n",
    "DATA_URL = \"https://d2hg8soec8ck9v.cloudfront.net/datasets/rugged_data.csv\"\n",
    "data = pd.read_csv(DATA_URL, encoding=\"ISO-8859-1\")\n",
    "df = data[[\"cont_africa\", \"rugged\", \"rgdppc_2000\"]]\n",
    "df = df[np.isfinite(df.rgdppc_2000)]\n",
    "df[\"rgdppc_2000\"] = np.log(df[\"rgdppc_2000\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cont_africa</th>\n",
       "      <th>rugged</th>\n",
       "      <th>rgdppc_2000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.858</td>\n",
       "      <td>7.492609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3.427</td>\n",
       "      <td>8.216929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.769</td>\n",
       "      <td>9.933263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.775</td>\n",
       "      <td>9.407032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2.688</td>\n",
       "      <td>7.792343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cont_africa  rugged  rgdppc_2000\n",
       "2            1   0.858     7.492609\n",
       "4            0   3.427     8.216929\n",
       "7            0   0.769     9.933263\n",
       "8            0   0.775     9.407032\n",
       "9            0   2.688     7.792343"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 模型构建\n",
    "\n",
    "在构建模型之前，我需要进一步熟悉Pyro，从头至尾用因果模型学习一遍。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Bayesian 模型 \n",
    "\n",
    "$$\n",
    "\\text{weight} \\sim N(\\mu, 1) \\\\\n",
    "\\text{measurement}|\\text{weight} \\sim N(\\text{weight}, 0.75)\n",
    "$$\n",
    "\n",
    "那么给定观测 $\\text{measurement} = 9.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a =  9.107474327087402\n",
      "b =  0.6285384893417358\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.infer\n",
    "import pyro.optim\n",
    "import pyro.distributions as dist\n",
    "\n",
    "pyro.set_rng_seed(101)\n",
    "mu = 8.5\n",
    "\n",
    "def scale(mu):\n",
    "    weight = pyro.sample(\"weight\", dist.Normal(mu, 1.0))\n",
    "    return pyro.sample(\"measurement\", dist.Normal(weight, 0.75))\n",
    "conditioned_scale = pyro.condition(scale, data={\"measurement\": 9.5})\n",
    "\n",
    "def scale_parametrized_guide(mu):\n",
    "    a, b = pyro.param(\"a\", torch.tensor(mu)), pyro.param(\"b\", torch.tensor(1.))\n",
    "    return pyro.sample(\"weight\", dist.Normal(a, torch.abs(b)))\n",
    "\n",
    "pyro.clear_param_store()\n",
    "svi = pyro.infer.SVI(model=conditioned_scale,\n",
    "                     guide=scale_parametrized_guide,\n",
    "                     optim=pyro.optim.SGD({\"lr\": 0.001, \"momentum\":0.1}),\n",
    "                     loss=pyro.infer.Trace_ELBO())\n",
    "\n",
    "\n",
    "losses, a,b  = [], [], []\n",
    "num_steps = 2500\n",
    "for t in range(num_steps):\n",
    "    losses.append(svi.step(mu))\n",
    "    a.append(pyro.param(\"a\").item())\n",
    "    b.append(pyro.param(\"b\").item())\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.title(\"ELBO\")\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"loss\");\n",
    "print('a = ',pyro.param(\"a\").item())\n",
    "print('b = ', pyro.param(\"b\").item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### 几何分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "p = pyro.sample('p', pyro.distributions.Uniform(0, 1))\n",
    "def geometric(p, t=0):\n",
    "    # 该函数表示第 t 次失败之后, 继续尝试直到成功，返回总共失败次数。\n",
    "    x = pyro.sample(\"x_{}\".format(t), pyro.distributions.Bernoulli(p))\n",
    "    if x.item() == 1:\n",
    "        return t\n",
    "    else:\n",
    "        t_update = t+1\n",
    "        return 1 + geometric(p, t_update)\n",
    "\n",
    "print(geometric(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.8346), tensor(0.4489))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc, scale = 0.0, 1.0\n",
    "u = pyro.sample('U', pyro.distributions.Uniform(0, 1))\n",
    "y = pyro.sample('Y', pyro.distributions.Normal(loc, scale))\n",
    "u, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, tensor(-0.2728))"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc, scale = 0.0, 4.0\n",
    "obs = torch.tensor(1.0)\n",
    "def model(loc, scale):\n",
    "    t = 0\n",
    "    theta = pyro.param(\"theta\", torch.tensor(2.0))\n",
    "    while t < 100000:\n",
    "        t = t + 1\n",
    "        u = pyro.sample('u_{}'.format(t), pyro.distributions.Uniform(0, 1))\n",
    "        y = pyro.sample('y_{}'.format(t), pyro.distributions.Normal(loc, scale))\n",
    "        if y**2 < u**2 / theta:\n",
    "            # 此时需要条件化 y_t with obs\n",
    "            break            \n",
    "    return t, y\n",
    "\n",
    "x = model(loc, scale)\n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alpha_q', 'beta_q', 'theta']"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[item for item in pyro.get_param_store()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### 掷硬币案例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing inference with use_decaying_avg_baseline=True\n",
      "..\n",
      "Did 128 steps of inference.\n",
      "Final absolute errors for the two variational parameters were 0.7989 & 0.7426\n",
      "Doing inference with use_decaying_avg_baseline=False\n",
      "........\n",
      "Did 732 steps of inference.\n",
      "Final absolute errors for the two variational parameters were 0.7674 & 0.7989\n"
     ]
    }
   ],
   "source": [
    "import os, torch, pyro, sys\n",
    "import torch.distributions.constraints as constraints\n",
    "import pyro.distributions as dist\n",
    "import pyro.optim as optim\n",
    "from pyro.infer import SVI, TraceGraph_ELBO\n",
    "from pyro.distributions.testing.fakes import NonreparameterizedBeta\n",
    "assert pyro.__version__.startswith('1.3.0')\n",
    "pyro.enable_validation(True)\n",
    "\n",
    "class BernoulliBetaExample:\n",
    "    def __init__(self, max_steps):\n",
    "        self.max_steps = max_steps\n",
    "        self.alpha0 = 10.0\n",
    "        self.beta0 = 10.0\n",
    "        self.data = torch.zeros(10)\n",
    "        self.data[0:6] = torch.ones(6)\n",
    "        self.n_data = self.data.size(0)\n",
    "        self.alpha_n = self.data.sum() + self.alpha0\n",
    "        self.beta_n = - self.data.sum() + torch.tensor(self.beta0 + self.n_data)\n",
    "        self.alpha_q_0 = 15.0\n",
    "        self.beta_q_0 = 15.0\n",
    "\n",
    "    def model(self, use_decaying_avg_baseline):\n",
    "        f = pyro.sample(\"latent_fairness\", dist.Beta(self.alpha0, self.beta0))\n",
    "        with pyro.plate(\"data_plate\"):\n",
    "            pyro.sample(\"obs\", dist.Bernoulli(f), obs=self.data)\n",
    "    def guide(self, use_decaying_avg_baseline):\n",
    "        alpha_q = pyro.param(\"alpha_q\", torch.tensor(self.alpha_q_0), constraint=constraints.positive)\n",
    "        beta_q = pyro.param(\"beta_q\", torch.tensor(self.beta_q_0), constraint=constraints.positive)\n",
    "        baseline_dict = {'use_decaying_avg_baseline': use_decaying_avg_baseline, 'baseline_beta': 0.90}\n",
    "        pyro.sample(\"latent_fairness\", NonreparameterizedBeta(alpha_q, beta_q), infer=dict(baseline=baseline_dict))\n",
    "\n",
    "    def do_inference(self, use_decaying_avg_baseline, tolerance=0.80):\n",
    "        pyro.clear_param_store()\n",
    "        optimizer = optim.Adam({\"lr\": .0005, \"betas\": (0.93, 0.999)})\n",
    "        svi = SVI(self.model, self.guide, optimizer, loss=TraceGraph_ELBO())\n",
    "        print(\"Doing inference with use_decaying_avg_baseline=%s\" % use_decaying_avg_baseline)\n",
    "\n",
    "        param_abs_error = lambda name, target: torch.sum(torch.abs(target - pyro.param(name))).item()\n",
    "        for k in range(self.max_steps):\n",
    "            svi.step(use_decaying_avg_baseline)\n",
    "            if k % 100 == 0:\n",
    "                print('.', end='')\n",
    "                sys.stdout.flush()\n",
    "\n",
    "            alpha_error = param_abs_error(\"alpha_q\", self.alpha_n)\n",
    "            beta_error = param_abs_error(\"beta_q\", self.beta_n)\n",
    "\n",
    "            if alpha_error < tolerance and beta_error < tolerance:\n",
    "                break\n",
    "\n",
    "        print(\"\\nDid %d steps of inference.\" % k)\n",
    "        print((\"Final absolute errors for the two variational parameters \" +\n",
    "               \"were %.4f & %.4f\") % (alpha_error, beta_error))\n",
    "\n",
    "bbe = BernoulliBetaExample(max_steps=1000)\n",
    "bbe.do_inference(use_decaying_avg_baseline=True)\n",
    "bbe.do_inference(use_decaying_avg_baseline=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### 拒绝抽样模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "需要采样 $p(x) = c \\cdot g(x) \\cdot h(x), Y \\sim h(x), U\\sim U(0, 1)$. \n",
    "\n",
    "按照 Structural Causal Model(SCM) 结构因果方程的思维 , $U$ 的取值会影响 $X$ 的取值，然后这不是事实，$X$ 的具体取值的大小完全取决于 $Y$，但是其分布却与 $U$ 的分布有关，相当某种选择偏差导致的分布变化。 所以我们理解结构因果方程时候， 更新方程是 $X \\leftarrow Y, Y \\sim h(x)$，而 $U$ 控制是否更新。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"162pt\" height=\"44pt\"\n",
       " viewBox=\"0.00 0.00 162.11 44.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 40)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-40 158.11,-40 158.11,4 -4,4\"/>\n",
       "<!-- Y -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>Y</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\">Y</text>\n",
       "</g>\n",
       "<!-- X -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>X</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"127.11\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"127.11\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\">X</text>\n",
       "</g>\n",
       "<!-- Y&#45;&gt;X -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>Y&#45;&gt;X</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M54.03,-18C65.01,-18 77.98,-18 89.86,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"89.95,-21.5 99.95,-18 89.95,-14.5 89.95,-21.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"77.06\" y=\"-20.8\" font-family=\"Times,serif\" font-size=\"14.00\">U</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x132860fd0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Source\n",
    "Source('digraph{rankdir=LR; Y -> X[label=\"U\"]}')\n",
    "# ![reject.png](https://i.loli.net/2020/04/01/WyshxgO3X5moDTA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "具体的采样原理见：\n",
    "\n",
    "<img width=300 src=\"https://i.loli.net/2020/04/01/WyshxgO3X5moDTA.png\" > </img>\n",
    "\n",
    "具体算法如下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6984, tensor(-0.0415))"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc, scale = 10.0, 4.0\n",
    "obs = torch.tensor(1.0)\n",
    "def model(loc, scale):\n",
    "    t = 0\n",
    "    theta = 100\n",
    "    while t < 10000:\n",
    "        t = t + 1\n",
    "        u = pyro.sample('u_{}'.format(t), pyro.distributions.Uniform(0, 1))\n",
    "        y = pyro.sample('y_{}'.format(t), pyro.distributions.Normal(loc, scale))\n",
    "        if y**2 < u**2 / theta:\n",
    "            # 此时需要条件化 y_t with obs\n",
    "            break     \n",
    "    if t >= 10000:\n",
    "        print(\"Hit the max iteration!\")\n",
    "    return t, y\n",
    "\n",
    "x = model(loc, scale)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import os, torch, pyro, sys\n",
    "import torch.distributions.constraints as constraints\n",
    "import pyro.distributions as dist\n",
    "import pyro.optim as optim\n",
    "from pyro.infer import SVI, TraceGraph_ELBO\n",
    "from pyro.distributions.testing.fakes import NonreparameterizedBeta\n",
    "pyro.enable_validation(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing inference with use_decaying_avg_baseline=True\n",
      "..\n",
      "Did 186 steps of inference.\n",
      "Final absolute errors for the two variational parameters were 0.7994 & 0.7861\n",
      "Doing inference with use_decaying_avg_baseline=False\n",
      ".....\n",
      "Did 448 steps of inference.\n",
      "Final absolute errors for the two variational parameters were 0.7969 & 0.7989\n"
     ]
    }
   ],
   "source": [
    "class RejectSampling(object):\n",
    "    def __init__(self, max_steps):\n",
    "        self.max_steps = max_steps\n",
    "\n",
    "    def model(self, use_decaying_avg_baseline):\n",
    "        f = pyro.sample(\"control\", dist.Uniform(0, 1))\n",
    "        with pyro.plate(\"data_plate\"):\n",
    "            pyro.sample(\"obs\", dist.Bernoulli(f), obs=self.data)\n",
    "    def guide(self, use_decaying_avg_baseline):\n",
    "        alpha_q = pyro.param(\"alpha_q\", torch.tensor(self.alpha_q_0), constraint=constraints.positive)\n",
    "        beta_q = pyro.param(\"beta_q\", torch.tensor(self.beta_q_0), constraint=constraints.positive)\n",
    "        baseline_dict = {'use_decaying_avg_baseline': use_decaying_avg_baseline, 'baseline_beta': 0.90}\n",
    "        pyro.sample(\"latent_fairness\", NonreparameterizedBeta(alpha_q, beta_q), infer=dict(baseline=baseline_dict))\n",
    "\n",
    "    def do_inference(self, use_decaying_avg_baseline, tolerance=0.80):\n",
    "        pyro.clear_param_store()\n",
    "        optimizer = optim.Adam({\"lr\": .0005, \"betas\": (0.93, 0.999)})\n",
    "        svi = SVI(self.model, self.guide, optimizer, loss=TraceGraph_ELBO())\n",
    "        print(\"Doing inference with use_decaying_avg_baseline=%s\" % use_decaying_avg_baseline)\n",
    "\n",
    "        param_abs_error = lambda name, target: torch.sum(torch.abs(target - pyro.param(name))).item()\n",
    "        for k in range(self.max_steps):\n",
    "            svi.step(use_decaying_avg_baseline)\n",
    "            if k % 100 == 0:\n",
    "                print('.', end='')\n",
    "                sys.stdout.flush()\n",
    "\n",
    "            alpha_error = param_abs_error(\"alpha_q\", self.alpha_n)\n",
    "            beta_error = param_abs_error(\"beta_q\", self.beta_n)\n",
    "\n",
    "            if alpha_error < tolerance and beta_error < tolerance:\n",
    "                break\n",
    "\n",
    "        print(\"\\nDid %d steps of inference.\" % k)\n",
    "        print((\"Final absolute errors for the two variational parameters \" +\n",
    "               \"were %.4f & %.4f\") % (alpha_error, beta_error))\n",
    "\n",
    "g = BernoulliBetaExample(max_steps=1000)\n",
    "g.model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Mini-Pyro "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Mini-pyro 例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import torch\n",
    "\n",
    "from pyro.generic import distributions as dist\n",
    "# We use the pyro.generic interface to support dynamic choice of backend.\n",
    "from pyro.generic import infer, ops, optim, pyro, pyro_backend\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    # Define a basic model with a single Normal latent random variable `loc`\n",
    "    # and a batch of Normally distributed observations.\n",
    "    def model(data):\n",
    "        loc = pyro.sample(\"loc\", dist.Normal(0., 1.))\n",
    "        with pyro.plate(\"data\", len(data), dim=-1):\n",
    "            pyro.sample(\"obs\", dist.Normal(loc, 1.), obs=data)\n",
    "\n",
    "    # Define a guide (i.e. variational distribution) with a Normal\n",
    "    # distribution over the latent random variable `loc`.\n",
    "    def guide(data):\n",
    "        guide_loc = pyro.param(\"guide_loc\", torch.tensor(0.))\n",
    "        guide_scale = ops.exp(pyro.param(\"guide_scale_log\", torch.tensor(0.)))\n",
    "        pyro.sample(\"loc\", dist.Normal(guide_loc, guide_scale))\n",
    "\n",
    "    # Generate some data.\n",
    "    torch.manual_seed(0)\n",
    "    data = torch.randn(100) + 3.0\n",
    "\n",
    "    # Because the API in minipyro matches that of Pyro proper,\n",
    "    # training code works with generic Pyro implementations.\n",
    "    with pyro_backend(args.backend):\n",
    "        # Construct an SVI object so we can do variational inference on our\n",
    "        # model/guide pair.\n",
    "        Elbo = infer.JitTrace_ELBO if args.jit else infer.Trace_ELBO\n",
    "        elbo = Elbo()\n",
    "        adam = optim.Adam({\"lr\": args.learning_rate})\n",
    "        svi = infer.SVI(model, guide, adam, elbo)\n",
    "\n",
    "        # Basic training loop\n",
    "        pyro.get_param_store().clear()\n",
    "        for step in range(args.num_steps):\n",
    "            loss = svi.step(data)\n",
    "            if step % 100 == 0:\n",
    "                print(\"step {} loss = {}\".format(step, loss))\n",
    "\n",
    "        # Report the final values of the variational parameters\n",
    "        # in the guide after training.\n",
    "        for name in pyro.get_param_store():\n",
    "            value = pyro.param(name)\n",
    "            print(\"{} = {}\".format(name, value.detach().cpu().numpy()))\n",
    "\n",
    "        # For this simple (conjugate) model we know the exact posterior. In\n",
    "        # particular we know that the variational distribution should be\n",
    "        # centered near 3.0. So let's check this explicitly.\n",
    "        assert (pyro.param(\"guide_loc\") - 3.0).abs() < 0.1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### 计算对数似然"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import pyro.poutine as poutine\n",
    "import pyro.distributions as dist\n",
    "\n",
    "def make_log_joint(model):\n",
    "    def _log_joint(cond_data, *args, **kwargs):\n",
    "        conditioned_model = poutine.condition(model, data=cond_data)\n",
    "        trace = poutine.trace(conditioned_model).get_trace(*args, **kwargs)\n",
    "        return trace.log_prob_sum()\n",
    "    return _log_joint\n",
    "\n",
    "from pyro.poutine.trace_messenger import TraceMessenger\n",
    "from pyro.poutine.condition_messenger import ConditionMessenger \n",
    "\n",
    "def make_log_joint_2(model):\n",
    "    def _log_joint(cond_data, *args, **kwargs):\n",
    "        with TraceMessenger() as tracer:\n",
    "            with ConditionMessenger(data=cond_data):\n",
    "                model(*args, **kwargs)\n",
    "        \n",
    "        trace = tracer.trace\n",
    "        logp = 0.\n",
    "        for name, node in trace.nodes.items():\n",
    "            if node[\"type\"] == \"sample\":\n",
    "                if node[\"is_observed\"]:\n",
    "                    assert node[\"value\"] is cond_data[name]\n",
    "                logp = logp + node[\"fn\"].log_prob(node[\"value\"]).sum()\n",
    "        return logp\n",
    "    return _log_joint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-5.2637)\n"
     ]
    }
   ],
   "source": [
    "def weather():\n",
    "    cloudy = pyro.sample('cloudy', pyro.distributions.Bernoulli(0.3))\n",
    "    cloudy = 'cloudy' if cloudy.item() == 1.0 else 'sunny'\n",
    "    mean_temp = {'cloudy': 55.0, 'sunny': 75.0}[cloudy]\n",
    "    scale_temp = {'cloudy': 10.0, 'sunny': 15.0}[cloudy]\n",
    "    temp = pyro.sample('temp', pyro.distributions.Normal(mean_temp, scale_temp))\n",
    "    return cloudy, temp.item()\n",
    "\n",
    "weight_log_joint = make_log_joint(weather)\n",
    "print(weight_log_joint({\"temp\": torch.tensor(51)}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 tensor(0.0642)\n",
      "{'temp': tensor(51.5000)}\n",
      "{'y_8': tensor(51.5000)}\n",
      "tensor(-261.4502)\n"
     ]
    }
   ],
   "source": [
    "def model():\n",
    "    loc, scale = 0.0, 4.0\n",
    "    t = 0\n",
    "    theta = pyro.param(\"theta\", torch.tensor(2.0))\n",
    "    while t < 100000:\n",
    "        t = t + 1\n",
    "        u = pyro.sample('u_{}'.format(t), pyro.distributions.Uniform(0, 1))\n",
    "        y = pyro.sample('y_{}'.format(t), pyro.distributions.Normal(loc, scale))\n",
    "        if y**2 < u**2 / theta:\n",
    "            # 此时需要条件化 y_t with obs\n",
    "            break            \n",
    "    return t, y\n",
    "\n",
    "def make_log_joint(model):\n",
    "    t, y = model()\n",
    "    name = \"y_{}\".format(t)\n",
    "    print(t, y)\n",
    "    def _log_joint(cond_data, *args, **kwargs):\n",
    "        print(cond_data)\n",
    "        cond_data = {name:v for k, v in cond_data.items()}\n",
    "        print(cond_data)\n",
    "        conditioned_model = poutine.condition(model, data=cond_data)\n",
    "        trace = poutine.trace(conditioned_model).get_trace(*args, **kwargs)\n",
    "        return trace.log_prob_sum()\n",
    "    return _log_joint\n",
    "\n",
    "model_log_joint = make_log_joint(model)\n",
    "print(model_log_joint({\"temp\": torch.tensor(51.5)}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原始代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "Collapsed": "false",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('sunny', 100.9456787109375)\n",
      "('sunny', 85.012451171875)\n",
      "('sunny', 61.389869689941406)\n",
      "sunny 83.8826904296875\n",
      "{'temp': tensor(51)}\n",
      "{'y_sunny': tensor(51)}\n",
      "tensor(-4.2058)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error while computing log_prob_sum at site 'weight':\nThe value argument to log_prob must be a Tensor\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pyro/poutine/trace_struct.py\u001b[0m in \u001b[0;36mlog_prob_sum\u001b[0;34m(self, site_filter)\u001b[0m\n\u001b[1;32m    190\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                         \u001b[0mlog_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"fn\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"args\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"kwargs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/distributions/normal.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;31m# compute the variance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36m_validate_sample\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The value argument to log_prob must be a Tensor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The value argument to log_prob must be a Tensor",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-426-8813abdf5019>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mscale_log_joint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_log_joint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale_log_joint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"measurement\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m9.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"weight\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m8.23\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-426-8813abdf5019>\u001b[0m in \u001b[0;36m_log_joint\u001b[0;34m(cond_data, *args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mconditioned_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoutine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcond_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mtrace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoutine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconditioned_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_log_joint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pyro/poutine/trace_struct.py\u001b[0m in \u001b[0;36mlog_prob_sum\u001b[0;34m(self, site_filter)\u001b[0m\n\u001b[1;32m    194\u001b[0m                         \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_site\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                         raise ValueError(\"Error while computing log_prob_sum at site '{}':\\n{}\\n\"\n\u001b[0;32m--> 196\u001b[0;31m                                          .format(name, exc_value, shapes)).with_traceback(traceback)\n\u001b[0m\u001b[1;32m    197\u001b[0m                     \u001b[0mlog_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscale_and_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scale\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                     \u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"log_prob_sum\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_p\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pyro/poutine/trace_struct.py\u001b[0m in \u001b[0;36mlog_prob_sum\u001b[0;34m(self, site_filter)\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                         \u001b[0mlog_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"fn\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"args\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"kwargs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/distributions/normal.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;31m# compute the variance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36m_validate_sample\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \"\"\"\n\u001b[1;32m    237\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The value argument to log_prob must be a Tensor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mevent_dim_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error while computing log_prob_sum at site 'weight':\nThe value argument to log_prob must be a Tensor\n"
     ]
    }
   ],
   "source": [
    "def scale(mu):\n",
    "    weight = pyro.sample(\"weight\", dist.Normal(mu, 1.0))\n",
    "    out = pyro.sample(\"measurement\", dist.Normal(weight, 0.75))\n",
    "    return out\n",
    "\n",
    "def weather():\n",
    "    cloudy = pyro.sample('cloudy', pyro.distributions.Bernoulli(0.3))\n",
    "    cloudy = 'cloudy' if cloudy.item() == 1.0 else 'sunny'\n",
    "    mean_temp = {'cloudy': 55.0, 'sunny': 75.0}[cloudy]\n",
    "    scale_temp = {'cloudy': 10.0, 'sunny': 15.0}[cloudy]\n",
    "    temp = pyro.sample('temp', pyro.distributions.Normal(mean_temp, scale_temp))\n",
    "    return cloudy, temp.item()\n",
    "\n",
    "for _ in range(3):\n",
    "    print(weather())\n",
    "\n",
    "weight_log_joint = make_log_joint(weather)\n",
    "print(weight_log_joint({\"temp\": torch.tensor(51)}))\n",
    "\n",
    "import pyro.poutine as poutine\n",
    "import pyro.distributions as dist\n",
    "\n",
    "def make_log_joint(model):\n",
    "    def _log_joint(cond_data, *args, **kwargs):\n",
    "        conditioned_model = poutine.condition(model, data=cond_data)\n",
    "        trace = poutine.trace(conditioned_model).get_trace(*args, **kwargs)\n",
    "        return trace.log_prob_sum()\n",
    "    return _log_joint\n",
    "\n",
    "scale_log_joint = make_log_joint(scale)\n",
    "print(scale_log_joint({\"measurement\": 9.5, \"weight\": 8.23}, 8.5))\n",
    "\n",
    "def model():\n",
    "    loc, scale = 0.0, 4.0\n",
    "    t = 0\n",
    "    theta = pyro.param(\"theta\", torch.tensor(2.0))\n",
    "    while t < 100000:\n",
    "        t = t + 1\n",
    "        u = pyro.sample('u_{}'.format(t), pyro.distributions.Uniform(0, 1))\n",
    "        y = pyro.sample('y_{}'.format(t), pyro.distributions.Normal(loc, scale))\n",
    "        if y**2 < u**2 / theta:\n",
    "            # 此时需要条件化 y_t with obs\n",
    "            break            \n",
    "    return t, y\n",
    "\n",
    "def make_log_joint(model):\n",
    "    t, y = model()\n",
    "    name = \"y_{}\".format(t)\n",
    "    print(t, y)\n",
    "    def _log_joint(cond_data, *args, **kwargs):\n",
    "        print(cond_data)\n",
    "        cond_data = {name:v for k, v in cond_data.items()}\n",
    "        print(cond_data)\n",
    "        conditioned_model = poutine.condition(model, data=cond_data)\n",
    "        trace = poutine.trace(conditioned_model).get_trace(*args, **kwargs)\n",
    "        return trace.log_prob_sum()\n",
    "    return _log_joint\n",
    "\n",
    "model_log_joint = make_log_joint(model)\n",
    "print(model_log_joint({\"temp\": torch.tensor(51.5)}))\n",
    "\n",
    "from pyro.poutine.trace_messenger import TraceMessenger\n",
    "from pyro.poutine.condition_messenger import ConditionMessenger \n",
    "\n",
    "def make_log_joint_2(model):\n",
    "    def _log_joint(cond_data, *args, **kwargs):\n",
    "        with TraceMessenger() as tracer:\n",
    "            with ConditionMessenger(data=cond_data):\n",
    "                model(*args, **kwargs)\n",
    "        \n",
    "        trace = tracer.trace\n",
    "        logp = 0.\n",
    "        for name, node in trace.nodes.items():\n",
    "            if node[\"type\"] == \"sample\":\n",
    "                if node[\"is_observed\"]:\n",
    "                    assert node[\"value\"] is cond_data[name]\n",
    "                logp = logp + node[\"fn\"].log_prob(node[\"value\"]).sum()\n",
    "        return logp\n",
    "    return _log_joint\n",
    "\n",
    "scale_log_joint = make_log_joint_2(scale)\n",
    "print(scale_log_joint({\"measurement\": 9.5, \"weight\": 8.23}, 8.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
