{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "重要知识点：\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "- 动态控制随机函数\n",
    "\n",
    "这样随机函数有一个特点，那就是任何一个样本，它的维度是不确定的，你无法计算部分观测的概率。也是由于其样本是动态生成的，定义指导分布的时候，无法定义一个抽样语句与模型分布中每个隐变量对应。所以也无法用那些推断算法，需要想其他办法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# ICM with One Node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "一个节点的信息因果模型 with a loop 是最简单的。We consider an Info Causal Model(ICM) with the causal graph $G=(V, E)$ where $V = \\{X\\}$ and $E=\\{[X, X]\\}$. 我们本文的目的是希望用这个最简单的模型阐述清楚 ICM 的构建，训练，和推断(causal and statistical inference)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"80pt\" height=\"44pt\"\n",
       " viewBox=\"0.00 0.00 80.00 44.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 40)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-40 76,-40 76,4 -4,4\"/>\n",
       "<!-- X -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>X</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\">X</text>\n",
       "</g>\n",
       "<!-- X&#45;&gt;X -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>X&#45;&gt;X</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.44,-24.69C63.03,-25.15 72,-22.92 72,-18 72,-14.77 68.14,-12.7 62.49,-11.79\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.6,-8.29 52.44,-11.31 62.27,-15.28 62.6,-8.29\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x1115f0550>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Source\n",
    "Source('digraph{X->X}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "作为一个简单的 feedback system, 表面上看很简单，但是如果我们允许 $X$ 是一个复杂内部结构的张量，那么任何反馈系统都可以抽样成这种形式。因此我们需要研究在此简单模型上的因果推断。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "\n",
    "# for CI testing\n",
    "smoke_test = ('CI' in os.environ)\n",
    "assert pyro.__version__.startswith('1.3.0')\n",
    "pyro.enable_validation(True)\n",
    "pyro.set_rng_seed(1)\n",
    "pyro.enable_validation(True)\n",
    "\n",
    "\n",
    "# Set matplotlib settings\n",
    "%matplotlib inline\n",
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "## Data\n",
    "DATA_URL = \"https://d2hg8soec8ck9v.cloudfront.net/datasets/rugged_data.csv\"\n",
    "data = pd.read_csv(DATA_URL, encoding=\"ISO-8859-1\")\n",
    "df = data[[\"cont_africa\", \"rugged\", \"rgdppc_2000\"]]\n",
    "df = df[np.isfinite(df.rgdppc_2000)]\n",
    "df[\"rgdppc_2000\"] = np.log(df[\"rgdppc_2000\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cont_africa</th>\n",
       "      <th>rugged</th>\n",
       "      <th>rgdppc_2000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.858</td>\n",
       "      <td>7.492609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3.427</td>\n",
       "      <td>8.216929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.769</td>\n",
       "      <td>9.933263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.775</td>\n",
       "      <td>9.407032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2.688</td>\n",
       "      <td>7.792343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cont_africa  rugged  rgdppc_2000\n",
       "2            1   0.858     7.492609\n",
       "4            0   3.427     8.216929\n",
       "7            0   0.769     9.933263\n",
       "8            0   0.775     9.407032\n",
       "9            0   2.688     7.792343"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## model\n",
    "\n",
    "先把一个节点的模型搭建起来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import math, os, torch, pyro\n",
    "import torch.distributions.constraints as constraints\n",
    "import pyro.distributions as dist\n",
    "from pyro.optim import Adam\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "\n",
    "assert pyro.__version__.startswith('1.3.0')\n",
    "pyro.enable_validation(True)\n",
    "pyro.clear_param_store()\n",
    "\n",
    "data = []\n",
    "data.extend([torch.tensor(1.0) for _ in range(6)])\n",
    "data.extend([torch.tensor(0.0) for _ in range(4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"80pt\" height=\"44pt\"\n",
       " viewBox=\"0.00 0.00 80.00 44.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 40)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-40 76,-40 76,4 -4,4\"/>\n",
       "<!-- X -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>X</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\">X</text>\n",
       "</g>\n",
       "<!-- X&#45;&gt;X -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>X&#45;&gt;X</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.44,-24.69C63.03,-25.15 72,-22.92 72,-18 72,-14.77 68.14,-12.7 62.49,-11.79\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.6,-8.29 52.44,-11.31 62.27,-15.28 62.6,-8.29\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x1115f0550>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Source\n",
    "Source('digraph{X->X}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "我们发现 `model` 会调用多次，无法对最后一个随机变量条件化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1, preditive coding 0.0 and control 1.0\n",
      "step 2, updated preditive coding -1.2103749513626099 and control 1.0\n"
     ]
    }
   ],
   "source": [
    "def model():\n",
    "    x_c = torch.tensor(0.0)\n",
    "    alpha0, beta0 = torch.tensor(10.0), torch.tensor(10.0)\n",
    "    theta = pyro.sample(\"latent_fairness\", dist.Beta(alpha0, beta0))\n",
    "    \n",
    "    t = 1\n",
    "    alpha = 0.1\n",
    "    x = pyro.sample(\"sample_{}\".format(t), dist.Normal(x_c, 1))\n",
    "    # 第一个门控通过概率为 theta, 第二门控通过概率为 1.\n",
    "    B = pyro.sample('control_{}'.format(t), dist.Bernoulli(probs=theta))\n",
    "    print(\"step {}, preditive coding {} and control {}\".format(t, x_c.item(), B.item()))\n",
    "    \n",
    "    while B:\n",
    "        t = t + 1\n",
    "        x_c = alpha * x_c + (1 - alpha) * x # 更新预测编码\n",
    "        print(\"step {}, updated preditive coding {} and control {}\".format(t, x_c.item(), B.item()))\n",
    "        B = pyro.sample('control_{}'.format(t), dist.Bernoulli(probs=theta)) # 门控\n",
    "        x = pyro.sample(\"sample_{}\".format(t), dist.Normal(x_c, 1)) # 抽样\n",
    "\n",
    "    return x, t\n",
    "\n",
    "\n",
    "cond_data = {\"x_{}\".format(t): torch.tensor(1.0)}\n",
    "cond_model = poutine.condition(model, data={\"x_{}\".format(model()[-1]): torch.tensor(1.0)})\n",
    "# trace = poutine.trace(cond_model).get_trace()\n",
    "# trace.nodes\n",
    "# 想对最后一个 sample site 条件化，失败了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "但是，一种特殊情况是可以对最后一个变量条件化的，那就是门控独立于样本抽样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1, preditive coding 0.0 and control 1.0\n",
      "None\n",
      "Current x_c tensor(0.)\n",
      "step 2, updated preditive coding 1.7503198385238647 and control 1.0\n",
      "{'x_2': tensor(1.)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "odict_items([('_INPUT', {'name': '_INPUT', 'type': 'args', 'args': (tensor(1.),), 'kwargs': {}}), ('latent_fairness', {'type': 'sample', 'name': 'latent_fairness', 'fn': Beta(), 'is_observed': False, 'args': (), 'kwargs': {}, 'value': tensor(0.6435), 'infer': {}, 'scale': 1.0, 'mask': None, 'cond_indep_stack': (), 'done': True, 'stop': False, 'continuation': None}), ('sample_1', {'type': 'sample', 'name': 'sample_1', 'fn': Normal(loc: 0.0, scale: 1.0), 'is_observed': False, 'args': (), 'kwargs': {}, 'value': tensor(1.9448), 'infer': {}, 'scale': 1.0, 'mask': None, 'cond_indep_stack': (), 'done': True, 'stop': False, 'continuation': None}), ('control_1', {'type': 'sample', 'name': 'control_1', 'fn': Bernoulli(probs: 0.6434524655342102), 'is_observed': False, 'args': (), 'kwargs': {}, 'value': tensor(1.), 'infer': {}, 'scale': 1.0, 'mask': None, 'cond_indep_stack': (), 'done': True, 'stop': False, 'continuation': None}), ('control_2', {'type': 'sample', 'name': 'control_2', 'fn': Bernoulli(probs: 0.6434524655342102), 'is_observed': False, 'args': (), 'kwargs': {}, 'value': tensor(0.), 'infer': {}, 'scale': 1.0, 'mask': None, 'cond_indep_stack': (), 'done': True, 'stop': False, 'continuation': None}), ('sample_2', {'type': 'sample', 'name': 'sample_2', 'fn': Normal(loc: 1.7503198385238647, scale: 1.0), 'is_observed': True, 'args': (), 'kwargs': {}, 'value': {'x_2': tensor(1.)}, 'infer': {}, 'scale': 1.0, 'mask': None, 'cond_indep_stack': (), 'done': True, 'stop': False, 'continuation': None}), ('_RETURN', {'name': '_RETURN', 'type': 'return', 'value': ({'x_2': tensor(1.)}, 2)})])"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model(obs):\n",
    "    x_c = torch.tensor(0.0)\n",
    "    alpha0, beta0 = torch.tensor(10.0), torch.tensor(10.0)\n",
    "    theta = pyro.sample(\"latent_fairness\", dist.Beta(alpha0, beta0))\n",
    "    \n",
    "    t = 1\n",
    "    alpha = 0.1\n",
    "    x = pyro.sample(\"sample_{}\".format(t), dist.Normal(x_c, 1))\n",
    "    # 第一个门控通过概率为 theta, 第二门控通过概率为 1.\n",
    "    B = pyro.sample('control_{}'.format(t), dist.Bernoulli(probs=theta))\n",
    "    print(\"step {}, preditive coding {} and control {}\".format(t, x_c.item(), B.item()))\n",
    "    data = None if B else {\"x_{}\".format(t): obs}\n",
    "    print(data)\n",
    "    \n",
    "    while B:\n",
    "        t = t + 1\n",
    "        print('Current x_c', x_c)\n",
    "        x_c = alpha * x_c + (1 - alpha) * x # 更新预测编码\n",
    "        print(\"step {}, updated preditive coding {} and control {}\".format(t, x_c.item(), B.item()))\n",
    "        B = pyro.sample('control_{}'.format(t), dist.Bernoulli(probs=theta)) # 门控\n",
    "        \n",
    "        data = None if B else {\"x_{}\".format(t): torch.tensor(1.0)}\n",
    "        print(data)\n",
    "        x = pyro.sample(\"sample_{}\".format(t), dist.Normal(x_c, 1), obs=data) # 抽样\n",
    "\n",
    "    return x, t\n",
    "\n",
    "obs = torch.tensor(1.0)\n",
    "trace = poutine.trace(model).get_trace(obs)\n",
    "trace.nodes.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "这个随机函数有一个特点，那就是任何一个样本，它的维度是不确定的，你无法计算部分观测的概率。也是由于其样本是动态生成的，定义指导分布的时候，无法定义一个抽样语句与模型分布中每个隐变量对应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "{'x_2': tensor(52)}\n",
      "x_0 {'type': 'sample', 'name': 'x_0', 'fn': Bernoulli(probs: 0.5), 'is_observed': False, 'args': (), 'kwargs': {}, 'value': tensor(0.), 'infer': {}, 'scale': 1.0, 'mask': None, 'cond_indep_stack': (), 'done': True, 'stop': False, 'continuation': None}\n",
      "x_1 {'type': 'sample', 'name': 'x_1', 'fn': Bernoulli(probs: 0.5), 'is_observed': False, 'args': (), 'kwargs': {}, 'value': tensor(0.), 'infer': {}, 'scale': 1.0, 'mask': None, 'cond_indep_stack': (), 'done': True, 'stop': False, 'continuation': None}\n",
      "x_2 {'type': 'sample', 'name': 'x_2', 'fn': Bernoulli(probs: 0.5), 'is_observed': False, 'args': (), 'kwargs': {}, 'value': tensor(1.), 'infer': {}, 'scale': 1.0, 'mask': None, 'cond_indep_stack': (), 'done': True, 'stop': False, 'continuation': None}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-2.0794)"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyro.poutine as poutine\n",
    "import pyro.distributions as dist\n",
    "from pyro.poutine.trace_messenger import TraceMessenger\n",
    "from pyro.poutine.condition_messenger import ConditionMessenger\n",
    "\n",
    "def geometric(p, t=None):\n",
    "    t = 0 if t is None else t\n",
    "    # 该函数表示第 t 次失败之后, 继续尝试直到成功，返回总共失败次数。\n",
    "    x = pyro.sample(\"x_{}\".format(t), pyro.distributions.Bernoulli(p))\n",
    "    if x.item() == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        t_update = t+1\n",
    "        return 1+geometric(p, t_update)\n",
    "\n",
    "print(geometric(0.1))\n",
    "\n",
    "cond_data = {\"last\": torch.tensor(52)}\n",
    "\n",
    "with TraceMessenger() as tracer:\n",
    "    cond_data = {\"x_{}\".format(geometric(0.5)):v for k, v in cond_data.items()}\n",
    "\n",
    "print(cond_data)\n",
    "trace = tracer.trace\n",
    "logp = 0.\n",
    "for name, node in trace.nodes.items():\n",
    "    print(name, node)\n",
    "    if node[\"type\"] == \"sample\":\n",
    "        if node[\"is_observed\"]:\n",
    "            assert node[\"value\"] is cond_data[name]\n",
    "            print('观测变量', node)\n",
    "        logp = logp + node[\"fn\"].log_prob(node[\"value\"]).sum()\n",
    "\n",
    "logp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### 尝试一:门控独立于样本 \n",
    "\n",
    "模型是这样的： theta 是门控的概率，具备先验分布 `dist.Beta(alpha0, beta0)`.\n",
    "\n",
    "有了一个数据之后，请问 $theta$ 的后验分布是什么。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1, preditive coding 0.0 and control 1.0\n",
      "None\n",
      "Current x_c tensor(0.)\n",
      "step 2, updated preditive coding 0.5402063727378845 and control 1.0\n",
      "{'x_2': tensor(1.)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "odict_items([('_INPUT', {'name': '_INPUT', 'type': 'args', 'args': (tensor(1.),), 'kwargs': {}}), ('latent_fairness', {'type': 'sample', 'name': 'latent_fairness', 'fn': Beta(), 'is_observed': False, 'args': (), 'kwargs': {}, 'value': tensor(0.3170), 'infer': {}, 'scale': 1.0, 'mask': None, 'cond_indep_stack': (), 'done': True, 'stop': False, 'continuation': None}), ('sample_1', {'type': 'sample', 'name': 'sample_1', 'fn': Normal(loc: 0.0, scale: 1.0), 'is_observed': False, 'args': (), 'kwargs': {}, 'value': tensor(0.6002), 'infer': {}, 'scale': 1.0, 'mask': None, 'cond_indep_stack': (), 'done': True, 'stop': False, 'continuation': None}), ('control_1', {'type': 'sample', 'name': 'control_1', 'fn': Bernoulli(probs: 0.31696274876594543), 'is_observed': False, 'args': (), 'kwargs': {}, 'value': tensor(1.), 'infer': {}, 'scale': 1.0, 'mask': None, 'cond_indep_stack': (), 'done': True, 'stop': False, 'continuation': None}), ('control_2', {'type': 'sample', 'name': 'control_2', 'fn': Bernoulli(probs: 0.31696274876594543), 'is_observed': False, 'args': (), 'kwargs': {}, 'value': tensor(0.), 'infer': {}, 'scale': 1.0, 'mask': None, 'cond_indep_stack': (), 'done': True, 'stop': False, 'continuation': None}), ('sample_2', {'type': 'sample', 'name': 'sample_2', 'fn': Normal(loc: 0.5402063727378845, scale: 1.0), 'is_observed': True, 'args': (), 'kwargs': {}, 'value': {'x_2': tensor(1.)}, 'infer': {}, 'scale': 1.0, 'mask': None, 'cond_indep_stack': (), 'done': True, 'stop': False, 'continuation': None}), ('_RETURN', {'name': '_RETURN', 'type': 'return', 'value': ({'x_2': tensor(1.)}, 2)})])"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model(obs):\n",
    "    x_c = torch.tensor(0.0)\n",
    "    alpha0, beta0 = torch.tensor(10.0), torch.tensor(10.0)\n",
    "    theta = pyro.sample(\"latent_fairness\", dist.Beta(alpha0, beta0))\n",
    "    \n",
    "    t = 1\n",
    "    alpha = 0.1\n",
    "    x = pyro.sample(\"sample_{}\".format(t), dist.Normal(x_c, 1))\n",
    "    # 第一个门控通过概率为 theta, 第二门控通过概率为 1.\n",
    "    B = pyro.sample('control_{}'.format(t), dist.Bernoulli(probs=theta))\n",
    "    print(\"step {}, preditive coding {} and control {}\".format(t, x_c.item(), B.item()))\n",
    "    data = None if B else {\"x_{}\".format(t): obs}\n",
    "    print(data)\n",
    "    \n",
    "    while B:\n",
    "        t = t + 1\n",
    "        print('Current x_c', x_c)\n",
    "        x_c = alpha * x_c + (1 - alpha) * x # 更新预测编码\n",
    "        print(\"step {}, updated preditive coding {} and control {}\".format(t, x_c.item(), B.item()))\n",
    "        B = pyro.sample('control_{}'.format(t), dist.Bernoulli(probs=theta)) # 门控\n",
    "        \n",
    "        data = None if B else {\"x_{}\".format(t): torch.tensor(1.0)}\n",
    "        print(data)\n",
    "        x = pyro.sample(\"sample_{}\".format(t), dist.Normal(x_c, 1), obs=data) # 抽样\n",
    "\n",
    "    return x, t\n",
    "\n",
    "obs = torch.tensor(1.0)\n",
    "trace = poutine.trace(model).get_trace(obs)\n",
    "trace.nodes.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1, preditive coding 0.0 and control 1.0\n",
      "None\n",
      "Current x_c tensor(0.)\n",
      "step 2, updated preditive coding -0.12902162969112396 and control 1.0\n",
      "None\n",
      "Current x_c tensor(-0.1290)\n",
      "step 3, updated preditive coding -0.7264322638511658 and control 1.0\n",
      "{'x_3': tensor(1.)}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error while computing log_prob at site 'sample_3':\nThe value argument to log_prob must be a Tensor\n       Trace Shapes:  \n        Param Sites:  \n       Sample Sites:  \nlatent_fairness dist |\n               value |\n            log_prob |\n       sample_1 dist |\n               value |\n            log_prob |\n      control_1 dist |\n               value |\n            log_prob |\n      control_2 dist |\n               value |\n            log_prob |\n       sample_2 dist |\n               value |\n            log_prob |\n      control_3 dist |\n               value |\n            log_prob |\n       sample_3 dist |\n               value |",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pyro/poutine/trace_struct.py\u001b[0m in \u001b[0;36mcompute_log_prob\u001b[0;34m(self, site_filter)\u001b[0m\n\u001b[1;32m    215\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                         \u001b[0mlog_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"fn\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"args\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"kwargs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/distributions/normal.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;31m# compute the variance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36m_validate_sample\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The value argument to log_prob must be a Tensor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The value argument to log_prob must be a Tensor",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-333-10e9acbd97f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mn_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0msvi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pyro/infer/svi.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;31m# get loss and compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mpoutine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mparam_capture\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_and_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mguide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         params = set(site[\"value\"].unconstrained()\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pyro/infer/trace_elbo.py\u001b[0m in \u001b[0;36mloss_and_grads\u001b[0;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# grab a trace from the generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmodel_trace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguide_trace\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_traces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m             \u001b[0mloss_particle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msurrogate_loss_particle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_differentiable_loss_particle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_trace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguide_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_particle\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_particles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pyro/infer/elbo.py\u001b[0m in \u001b[0;36m_get_traces\u001b[0;34m(self, model, guide, args, kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_particles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pyro/infer/trace_elbo.py\u001b[0m in \u001b[0;36m_get_trace\u001b[0;34m(self, model, guide, args, kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \"\"\"\n\u001b[1;32m     52\u001b[0m         model_trace, guide_trace = get_importance_trace(\n\u001b[0;32m---> 53\u001b[0;31m             \"flat\", self.max_plate_nesting, model, guide, args, kwargs)\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_validation_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mcheck_if_enumerated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mguide_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pyro/infer/enum.py\u001b[0m in \u001b[0;36mget_importance_trace\u001b[0;34m(graph_type, max_plate_nesting, model, guide, args, kwargs, detach)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mmodel_trace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprune_subsample_sites\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mmodel_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mguide_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_score_parts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_validation_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pyro/poutine/trace_struct.py\u001b[0m in \u001b[0;36mcompute_log_prob\u001b[0;34m(self, site_filter)\u001b[0m\n\u001b[1;32m    219\u001b[0m                         \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_site\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                         raise ValueError(\"Error while computing log_prob at site '{}':\\n{}\\n{}\"\n\u001b[0;32m--> 221\u001b[0;31m                                          .format(name, exc_value, shapes)).with_traceback(traceback)\n\u001b[0m\u001b[1;32m    222\u001b[0m                     \u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"unscaled_log_prob\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_p\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m                     \u001b[0mlog_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscale_and_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scale\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pyro/poutine/trace_struct.py\u001b[0m in \u001b[0;36mcompute_log_prob\u001b[0;34m(self, site_filter)\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"log_prob\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                         \u001b[0mlog_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"fn\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"args\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"kwargs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/distributions/normal.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;31m# compute the variance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36m_validate_sample\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \"\"\"\n\u001b[1;32m    237\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The value argument to log_prob must be a Tensor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mevent_dim_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error while computing log_prob at site 'sample_3':\nThe value argument to log_prob must be a Tensor\n       Trace Shapes:  \n        Param Sites:  \n       Sample Sites:  \nlatent_fairness dist |\n               value |\n            log_prob |\n       sample_1 dist |\n               value |\n            log_prob |\n      control_1 dist |\n               value |\n            log_prob |\n      control_2 dist |\n               value |\n            log_prob |\n       sample_2 dist |\n               value |\n            log_prob |\n      control_3 dist |\n               value |\n            log_prob |\n       sample_3 dist |\n               value |"
     ]
    }
   ],
   "source": [
    "def guide(obs):\n",
    "    alpha_q = pyro.param(\"alpha_q\", torch.tensor(15.0), constraint=constraints.positive)\n",
    "    beta_q = pyro.param(\"beta_q\", torch.tensor(15.0), constraint=constraints.positive)\n",
    "    pyro.sample(\"latent_fairness\", dist.Beta(alpha_q, beta_q))\n",
    "    \n",
    "adam_params = {\"lr\": 0.0005, \"betas\": (0.90, 0.999)}\n",
    "optimizer = Adam(adam_params)\n",
    "\n",
    "svi = SVI(model, guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "n_steps = 2000\n",
    "for step in range(n_steps):\n",
    "    svi.step(obs)\n",
    "    if step % 100 == 0:\n",
    "        print('.', end='')\n",
    "\n",
    "alpha_q = pyro.param(\"alpha_q\").item()\n",
    "beta_q = pyro.param(\"beta_q\").item()\n",
    "inferred_mean = alpha_q / (alpha_q + beta_q)\n",
    "factor = beta_q / (alpha_q * (1.0 + alpha_q + beta_q))\n",
    "inferred_std = inferred_mean * math.sqrt(factor)\n",
    "print(\"\\nbased on the data and our prior belief, the fairness \" +\n",
    "      \"of the coin is %.3f +- %.3f\" % (inferred_mean, inferred_std))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true",
    "toc-hr-collapsed": true
   },
   "source": [
    "## 模型构建\n",
    "\n",
    "在构建模型之前，我需要进一步熟悉Pyro，从头至尾用因果模型学习一遍。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Bayesian 模型 \n",
    "\n",
    "$$\n",
    "\\text{weight} \\sim N(\\mu, 1) \\\\\n",
    "\\text{measurement}|\\text{weight} \\sim N(\\text{weight}, 0.75)\n",
    "$$\n",
    "\n",
    "那么给定观测 $\\text{measurement} = 9.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a =  9.107474327087402\n",
      "b =  0.6285384893417358\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.infer\n",
    "import pyro.optim\n",
    "import pyro.distributions as dist\n",
    "\n",
    "pyro.set_rng_seed(101)\n",
    "mu = 8.5\n",
    "\n",
    "def scale(mu):\n",
    "    weight = pyro.sample(\"weight\", dist.Normal(mu, 1.0))\n",
    "    return pyro.sample(\"measurement\", dist.Normal(weight, 0.75))\n",
    "conditioned_scale = pyro.condition(scale, data={\"measurement\": 9.5})\n",
    "\n",
    "def scale_parametrized_guide(mu):\n",
    "    a, b = pyro.param(\"a\", torch.tensor(mu)), pyro.param(\"b\", torch.tensor(1.))\n",
    "    return pyro.sample(\"weight\", dist.Normal(a, torch.abs(b)))\n",
    "\n",
    "pyro.clear_param_store()\n",
    "svi = pyro.infer.SVI(model=conditioned_scale,\n",
    "                     guide=scale_parametrized_guide,\n",
    "                     optim=pyro.optim.SGD({\"lr\": 0.001, \"momentum\":0.1}),\n",
    "                     loss=pyro.infer.Trace_ELBO())\n",
    "\n",
    "\n",
    "losses, a,b  = [], [], []\n",
    "num_steps = 2500\n",
    "for t in range(num_steps):\n",
    "    losses.append(svi.step(mu))\n",
    "    a.append(pyro.param(\"a\").item())\n",
    "    b.append(pyro.param(\"b\").item())\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.title(\"ELBO\")\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"loss\");\n",
    "print('a = ',pyro.param(\"a\").item())\n",
    "print('b = ', pyro.param(\"b\").item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### 几何分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "p = pyro.sample('p', pyro.distributions.Uniform(0, 1))\n",
    "def geometric(p, t=0):\n",
    "    # 该函数表示第 t 次失败之后, 继续尝试直到成功，返回总共失败次数。\n",
    "    x = pyro.sample(\"x_{}\".format(t), pyro.distributions.Bernoulli(p))\n",
    "    if x.item() == 1:\n",
    "        return t\n",
    "    else:\n",
    "        t_update = t+1\n",
    "        return 1 + geometric(p, t_update)\n",
    "\n",
    "print(geometric(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.8346), tensor(0.4489))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc, scale = 0.0, 1.0\n",
    "u = pyro.sample('U', pyro.distributions.Uniform(0, 1))\n",
    "y = pyro.sample('Y', pyro.distributions.Normal(loc, scale))\n",
    "u, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, tensor(-0.2728))"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc, scale = 0.0, 4.0\n",
    "obs = torch.tensor(1.0)\n",
    "def model(loc, scale):\n",
    "    t = 0\n",
    "    theta = pyro.param(\"theta\", torch.tensor(2.0))\n",
    "    while t < 100000:\n",
    "        t = t + 1\n",
    "        u = pyro.sample('u_{}'.format(t), pyro.distributions.Uniform(0, 1))\n",
    "        y = pyro.sample('y_{}'.format(t), pyro.distributions.Normal(loc, scale))\n",
    "        if y**2 < u**2 / theta:\n",
    "            # 此时需要条件化 y_t with obs\n",
    "            break            \n",
    "    return t, y\n",
    "\n",
    "x = model(loc, scale)\n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alpha_q', 'beta_q', 'theta']"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[item for item in pyro.get_param_store()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### 掷硬币案例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing inference with use_decaying_avg_baseline=True\n",
      "..\n",
      "Did 128 steps of inference.\n",
      "Final absolute errors for the two variational parameters were 0.7989 & 0.7426\n",
      "Doing inference with use_decaying_avg_baseline=False\n",
      "........\n",
      "Did 732 steps of inference.\n",
      "Final absolute errors for the two variational parameters were 0.7674 & 0.7989\n"
     ]
    }
   ],
   "source": [
    "import os, torch, pyro, sys\n",
    "import torch.distributions.constraints as constraints\n",
    "import pyro.distributions as dist\n",
    "import pyro.optim as optim\n",
    "from pyro.infer import SVI, TraceGraph_ELBO\n",
    "from pyro.distributions.testing.fakes import NonreparameterizedBeta\n",
    "assert pyro.__version__.startswith('1.3.0')\n",
    "pyro.enable_validation(True)\n",
    "\n",
    "class BernoulliBetaExample:\n",
    "    def __init__(self, max_steps):\n",
    "        self.max_steps = max_steps\n",
    "        self.alpha0 = 10.0\n",
    "        self.beta0 = 10.0\n",
    "        self.data = torch.zeros(10)\n",
    "        self.data[0:6] = torch.ones(6)\n",
    "        self.n_data = self.data.size(0)\n",
    "        self.alpha_n = self.data.sum() + self.alpha0\n",
    "        self.beta_n = - self.data.sum() + torch.tensor(self.beta0 + self.n_data)\n",
    "        self.alpha_q_0 = 15.0\n",
    "        self.beta_q_0 = 15.0\n",
    "\n",
    "    def model(self, use_decaying_avg_baseline):\n",
    "        f = pyro.sample(\"latent_fairness\", dist.Beta(self.alpha0, self.beta0))\n",
    "        with pyro.plate(\"data_plate\"):\n",
    "            pyro.sample(\"obs\", dist.Bernoulli(f), obs=self.data)\n",
    "    def guide(self, use_decaying_avg_baseline):\n",
    "        alpha_q = pyro.param(\"alpha_q\", torch.tensor(self.alpha_q_0), constraint=constraints.positive)\n",
    "        beta_q = pyro.param(\"beta_q\", torch.tensor(self.beta_q_0), constraint=constraints.positive)\n",
    "        baseline_dict = {'use_decaying_avg_baseline': use_decaying_avg_baseline, 'baseline_beta': 0.90}\n",
    "        pyro.sample(\"latent_fairness\", NonreparameterizedBeta(alpha_q, beta_q), infer=dict(baseline=baseline_dict))\n",
    "\n",
    "    def do_inference(self, use_decaying_avg_baseline, tolerance=0.80):\n",
    "        pyro.clear_param_store()\n",
    "        optimizer = optim.Adam({\"lr\": .0005, \"betas\": (0.93, 0.999)})\n",
    "        svi = SVI(self.model, self.guide, optimizer, loss=TraceGraph_ELBO())\n",
    "        print(\"Doing inference with use_decaying_avg_baseline=%s\" % use_decaying_avg_baseline)\n",
    "\n",
    "        param_abs_error = lambda name, target: torch.sum(torch.abs(target - pyro.param(name))).item()\n",
    "        for k in range(self.max_steps):\n",
    "            svi.step(use_decaying_avg_baseline)\n",
    "            if k % 100 == 0:\n",
    "                print('.', end='')\n",
    "                sys.stdout.flush()\n",
    "\n",
    "            alpha_error = param_abs_error(\"alpha_q\", self.alpha_n)\n",
    "            beta_error = param_abs_error(\"beta_q\", self.beta_n)\n",
    "\n",
    "            if alpha_error < tolerance and beta_error < tolerance:\n",
    "                break\n",
    "\n",
    "        print(\"\\nDid %d steps of inference.\" % k)\n",
    "        print((\"Final absolute errors for the two variational parameters \" +\n",
    "               \"were %.4f & %.4f\") % (alpha_error, beta_error))\n",
    "\n",
    "bbe = BernoulliBetaExample(max_steps=1000)\n",
    "bbe.do_inference(use_decaying_avg_baseline=True)\n",
    "bbe.do_inference(use_decaying_avg_baseline=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### 拒绝抽样模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "需要采样 $p(x) = c \\cdot g(x) \\cdot h(x), Y \\sim h(x), U\\sim U(0, 1)$. \n",
    "\n",
    "按照 Structural Causal Model(SCM) 结构因果方程的思维 , $U$ 的取值会影响 $X$ 的取值，然后这不是事实，$X$ 的具体取值的大小完全取决于 $Y$，但是其分布却与 $U$ 的分布有关，相当某种选择偏差导致的分布变化。 所以我们理解结构因果方程时候， 更新方程是 $X \\leftarrow Y, Y \\sim h(x)$，而 $U$ 控制是否更新。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"162pt\" height=\"44pt\"\n",
       " viewBox=\"0.00 0.00 162.11 44.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 40)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-40 158.11,-40 158.11,4 -4,4\"/>\n",
       "<!-- Y -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>Y</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\">Y</text>\n",
       "</g>\n",
       "<!-- X -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>X</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"127.11\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"127.11\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\">X</text>\n",
       "</g>\n",
       "<!-- Y&#45;&gt;X -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>Y&#45;&gt;X</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M54.03,-18C65.01,-18 77.98,-18 89.86,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"89.95,-21.5 99.95,-18 89.95,-14.5 89.95,-21.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"77.06\" y=\"-20.8\" font-family=\"Times,serif\" font-size=\"14.00\">U</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x132860fd0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Source\n",
    "Source('digraph{rankdir=LR; Y -> X[label=\"U\"]}')\n",
    "# ![reject.png](https://i.loli.net/2020/04/01/WyshxgO3X5moDTA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "具体的采样原理见：\n",
    "\n",
    "<img width=300 src=\"https://i.loli.net/2020/04/01/WyshxgO3X5moDTA.png\" > </img>\n",
    "\n",
    "具体算法如下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6984, tensor(-0.0415))"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc, scale = 10.0, 4.0\n",
    "obs = torch.tensor(1.0)\n",
    "def model(loc, scale):\n",
    "    t = 0\n",
    "    theta = 100\n",
    "    while t < 10000:\n",
    "        t = t + 1\n",
    "        u = pyro.sample('u_{}'.format(t), pyro.distributions.Uniform(0, 1))\n",
    "        y = pyro.sample('y_{}'.format(t), pyro.distributions.Normal(loc, scale))\n",
    "        if y**2 < u**2 / theta:\n",
    "            # 此时需要条件化 y_t with obs\n",
    "            break     \n",
    "    if t >= 10000:\n",
    "        print(\"Hit the max iteration!\")\n",
    "    return t, y\n",
    "\n",
    "x = model(loc, scale)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import os, torch, pyro, sys\n",
    "import torch.distributions.constraints as constraints\n",
    "import pyro.distributions as dist\n",
    "import pyro.optim as optim\n",
    "from pyro.infer import SVI, TraceGraph_ELBO\n",
    "from pyro.distributions.testing.fakes import NonreparameterizedBeta\n",
    "pyro.enable_validation(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing inference with use_decaying_avg_baseline=True\n",
      "..\n",
      "Did 186 steps of inference.\n",
      "Final absolute errors for the two variational parameters were 0.7994 & 0.7861\n",
      "Doing inference with use_decaying_avg_baseline=False\n",
      ".....\n",
      "Did 448 steps of inference.\n",
      "Final absolute errors for the two variational parameters were 0.7969 & 0.7989\n"
     ]
    }
   ],
   "source": [
    "class RejectSampling(object):\n",
    "    def __init__(self, max_steps):\n",
    "        self.max_steps = max_steps\n",
    "\n",
    "    def model(self, use_decaying_avg_baseline):\n",
    "        f = pyro.sample(\"control\", dist.Uniform(0, 1))\n",
    "        with pyro.plate(\"data_plate\"):\n",
    "            pyro.sample(\"obs\", dist.Bernoulli(f), obs=self.data)\n",
    "    def guide(self, use_decaying_avg_baseline):\n",
    "        alpha_q = pyro.param(\"alpha_q\", torch.tensor(self.alpha_q_0), constraint=constraints.positive)\n",
    "        beta_q = pyro.param(\"beta_q\", torch.tensor(self.beta_q_0), constraint=constraints.positive)\n",
    "        baseline_dict = {'use_decaying_avg_baseline': use_decaying_avg_baseline, 'baseline_beta': 0.90}\n",
    "        pyro.sample(\"latent_fairness\", NonreparameterizedBeta(alpha_q, beta_q), infer=dict(baseline=baseline_dict))\n",
    "\n",
    "    def do_inference(self, use_decaying_avg_baseline, tolerance=0.80):\n",
    "        pyro.clear_param_store()\n",
    "        optimizer = optim.Adam({\"lr\": .0005, \"betas\": (0.93, 0.999)})\n",
    "        svi = SVI(self.model, self.guide, optimizer, loss=TraceGraph_ELBO())\n",
    "        print(\"Doing inference with use_decaying_avg_baseline=%s\" % use_decaying_avg_baseline)\n",
    "\n",
    "        param_abs_error = lambda name, target: torch.sum(torch.abs(target - pyro.param(name))).item()\n",
    "        for k in range(self.max_steps):\n",
    "            svi.step(use_decaying_avg_baseline)\n",
    "            if k % 100 == 0:\n",
    "                print('.', end='')\n",
    "                sys.stdout.flush()\n",
    "\n",
    "            alpha_error = param_abs_error(\"alpha_q\", self.alpha_n)\n",
    "            beta_error = param_abs_error(\"beta_q\", self.beta_n)\n",
    "\n",
    "            if alpha_error < tolerance and beta_error < tolerance:\n",
    "                break\n",
    "\n",
    "        print(\"\\nDid %d steps of inference.\" % k)\n",
    "        print((\"Final absolute errors for the two variational parameters \" +\n",
    "               \"were %.4f & %.4f\") % (alpha_error, beta_error))\n",
    "\n",
    "g = BernoulliBetaExample(max_steps=1000)\n",
    "g.model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "toc-hr-collapsed": true
   },
   "source": [
    "## Mini-Pyro "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Mini-pyro 例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import torch\n",
    "\n",
    "from pyro.generic import distributions as dist\n",
    "# We use the pyro.generic interface to support dynamic choice of backend.\n",
    "from pyro.generic import infer, ops, optim, pyro, pyro_backend\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    # Define a basic model with a single Normal latent random variable `loc`\n",
    "    # and a batch of Normally distributed observations.\n",
    "    def model(data):\n",
    "        loc = pyro.sample(\"loc\", dist.Normal(0., 1.))\n",
    "        with pyro.plate(\"data\", len(data), dim=-1):\n",
    "            pyro.sample(\"obs\", dist.Normal(loc, 1.), obs=data)\n",
    "\n",
    "    # Define a guide (i.e. variational distribution) with a Normal\n",
    "    # distribution over the latent random variable `loc`.\n",
    "    def guide(data):\n",
    "        guide_loc = pyro.param(\"guide_loc\", torch.tensor(0.))\n",
    "        guide_scale = ops.exp(pyro.param(\"guide_scale_log\", torch.tensor(0.)))\n",
    "        pyro.sample(\"loc\", dist.Normal(guide_loc, guide_scale))\n",
    "\n",
    "    # Generate some data.\n",
    "    torch.manual_seed(0)\n",
    "    data = torch.randn(100) + 3.0\n",
    "\n",
    "    # Because the API in minipyro matches that of Pyro proper,\n",
    "    # training code works with generic Pyro implementations.\n",
    "    with pyro_backend(args.backend):\n",
    "        # Construct an SVI object so we can do variational inference on our\n",
    "        # model/guide pair.\n",
    "        Elbo = infer.JitTrace_ELBO if args.jit else infer.Trace_ELBO\n",
    "        elbo = Elbo()\n",
    "        adam = optim.Adam({\"lr\": args.learning_rate})\n",
    "        svi = infer.SVI(model, guide, adam, elbo)\n",
    "\n",
    "        # Basic training loop\n",
    "        pyro.get_param_store().clear()\n",
    "        for step in range(args.num_steps):\n",
    "            loss = svi.step(data)\n",
    "            if step % 100 == 0:\n",
    "                print(\"step {} loss = {}\".format(step, loss))\n",
    "\n",
    "        # Report the final values of the variational parameters\n",
    "        # in the guide after training.\n",
    "        for name in pyro.get_param_store():\n",
    "            value = pyro.param(name)\n",
    "            print(\"{} = {}\".format(name, value.detach().cpu().numpy()))\n",
    "\n",
    "        # For this simple (conjugate) model we know the exact posterior. In\n",
    "        # particular we know that the variational distribution should be\n",
    "        # centered near 3.0. So let's check this explicitly.\n",
    "        assert (pyro.param(\"guide_loc\") - 3.0).abs() < 0.1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### 计算对数似然"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import pyro.poutine as poutine\n",
    "import pyro.distributions as dist\n",
    "\n",
    "def make_log_joint(model):\n",
    "    def _log_joint(cond_data, *args, **kwargs):\n",
    "        conditioned_model = poutine.condition(model, data=cond_data)\n",
    "        trace = poutine.trace(conditioned_model).get_trace(*args, **kwargs)\n",
    "        return trace.log_prob_sum()\n",
    "    return _log_joint\n",
    "\n",
    "from pyro.poutine.trace_messenger import TraceMessenger\n",
    "from pyro.poutine.condition_messenger import ConditionMessenger \n",
    "\n",
    "def make_log_joint_2(model):\n",
    "    def _log_joint(cond_data, *args, **kwargs):\n",
    "        with TraceMessenger() as tracer:\n",
    "            with ConditionMessenger(data=cond_data):\n",
    "                model(*args, **kwargs)\n",
    "        \n",
    "        trace = tracer.trace\n",
    "        logp = 0.\n",
    "        for name, node in trace.nodes.items():\n",
    "            if node[\"type\"] == \"sample\":\n",
    "                if node[\"is_observed\"]:\n",
    "                    assert node[\"value\"] is cond_data[name]\n",
    "                logp = logp + node[\"fn\"].log_prob(node[\"value\"]).sum()\n",
    "        return logp\n",
    "    return _log_joint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-5.2637)\n"
     ]
    }
   ],
   "source": [
    "def weather():\n",
    "    cloudy = pyro.sample('cloudy', pyro.distributions.Bernoulli(0.3))\n",
    "    cloudy = 'cloudy' if cloudy.item() == 1.0 else 'sunny'\n",
    "    mean_temp = {'cloudy': 55.0, 'sunny': 75.0}[cloudy]\n",
    "    scale_temp = {'cloudy': 10.0, 'sunny': 15.0}[cloudy]\n",
    "    temp = pyro.sample('temp', pyro.distributions.Normal(mean_temp, scale_temp))\n",
    "    return cloudy, temp.item()\n",
    "\n",
    "weight_log_joint = make_log_joint(weather)\n",
    "print(weight_log_joint({\"temp\": torch.tensor(51)}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 tensor(0.0642)\n",
      "{'temp': tensor(51.5000)}\n",
      "{'y_8': tensor(51.5000)}\n",
      "tensor(-261.4502)\n"
     ]
    }
   ],
   "source": [
    "def model():\n",
    "    loc, scale = 0.0, 4.0\n",
    "    t = 0\n",
    "    theta = pyro.param(\"theta\", torch.tensor(2.0))\n",
    "    while t < 100000:\n",
    "        t = t + 1\n",
    "        u = pyro.sample('u_{}'.format(t), pyro.distributions.Uniform(0, 1))\n",
    "        y = pyro.sample('y_{}'.format(t), pyro.distributions.Normal(loc, scale))\n",
    "        if y**2 < u**2 / theta:\n",
    "            # 此时需要条件化 y_t with obs\n",
    "            break            \n",
    "    return t, y\n",
    "\n",
    "def make_log_joint(model):\n",
    "    t, y = model()\n",
    "    name = \"y_{}\".format(t)\n",
    "    print(t, y)\n",
    "    def _log_joint(cond_data, *args, **kwargs):\n",
    "        print(cond_data)\n",
    "        cond_data = {name:v for k, v in cond_data.items()}\n",
    "        print(cond_data)\n",
    "        conditioned_model = poutine.condition(model, data=cond_data)\n",
    "        trace = poutine.trace(conditioned_model).get_trace(*args, **kwargs)\n",
    "        return trace.log_prob_sum()\n",
    "    return _log_joint\n",
    "\n",
    "model_log_joint = make_log_joint(model)\n",
    "print(model_log_joint({\"temp\": torch.tensor(51.5)}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "原始代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "Collapsed": "false",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('sunny', 100.9456787109375)\n",
      "('sunny', 85.012451171875)\n",
      "('sunny', 61.389869689941406)\n",
      "sunny 83.8826904296875\n",
      "{'temp': tensor(51)}\n",
      "{'y_sunny': tensor(51)}\n",
      "tensor(-4.2058)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error while computing log_prob_sum at site 'weight':\nThe value argument to log_prob must be a Tensor\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pyro/poutine/trace_struct.py\u001b[0m in \u001b[0;36mlog_prob_sum\u001b[0;34m(self, site_filter)\u001b[0m\n\u001b[1;32m    190\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                         \u001b[0mlog_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"fn\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"args\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"kwargs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/distributions/normal.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;31m# compute the variance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36m_validate_sample\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The value argument to log_prob must be a Tensor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The value argument to log_prob must be a Tensor",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-426-8813abdf5019>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mscale_log_joint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_log_joint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale_log_joint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"measurement\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m9.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"weight\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m8.23\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-426-8813abdf5019>\u001b[0m in \u001b[0;36m_log_joint\u001b[0;34m(cond_data, *args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mconditioned_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoutine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcond_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mtrace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoutine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconditioned_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_log_joint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pyro/poutine/trace_struct.py\u001b[0m in \u001b[0;36mlog_prob_sum\u001b[0;34m(self, site_filter)\u001b[0m\n\u001b[1;32m    194\u001b[0m                         \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_site\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                         raise ValueError(\"Error while computing log_prob_sum at site '{}':\\n{}\\n\"\n\u001b[0;32m--> 196\u001b[0;31m                                          .format(name, exc_value, shapes)).with_traceback(traceback)\n\u001b[0m\u001b[1;32m    197\u001b[0m                     \u001b[0mlog_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscale_and_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scale\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                     \u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"log_prob_sum\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_p\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pyro/poutine/trace_struct.py\u001b[0m in \u001b[0;36mlog_prob_sum\u001b[0;34m(self, site_filter)\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                         \u001b[0mlog_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"fn\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"args\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"kwargs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/distributions/normal.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;31m# compute the variance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36m_validate_sample\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \"\"\"\n\u001b[1;32m    237\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The value argument to log_prob must be a Tensor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mevent_dim_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error while computing log_prob_sum at site 'weight':\nThe value argument to log_prob must be a Tensor\n"
     ]
    }
   ],
   "source": [
    "def scale(mu):\n",
    "    weight = pyro.sample(\"weight\", dist.Normal(mu, 1.0))\n",
    "    out = pyro.sample(\"measurement\", dist.Normal(weight, 0.75))\n",
    "    return out\n",
    "\n",
    "def weather():\n",
    "    cloudy = pyro.sample('cloudy', pyro.distributions.Bernoulli(0.3))\n",
    "    cloudy = 'cloudy' if cloudy.item() == 1.0 else 'sunny'\n",
    "    mean_temp = {'cloudy': 55.0, 'sunny': 75.0}[cloudy]\n",
    "    scale_temp = {'cloudy': 10.0, 'sunny': 15.0}[cloudy]\n",
    "    temp = pyro.sample('temp', pyro.distributions.Normal(mean_temp, scale_temp))\n",
    "    return cloudy, temp.item()\n",
    "\n",
    "for _ in range(3):\n",
    "    print(weather())\n",
    "\n",
    "weight_log_joint = make_log_joint(weather)\n",
    "print(weight_log_joint({\"temp\": torch.tensor(51)}))\n",
    "\n",
    "import pyro.poutine as poutine\n",
    "import pyro.distributions as dist\n",
    "\n",
    "def make_log_joint(model):\n",
    "    def _log_joint(cond_data, *args, **kwargs):\n",
    "        conditioned_model = poutine.condition(model, data=cond_data)\n",
    "        trace = poutine.trace(conditioned_model).get_trace(*args, **kwargs)\n",
    "        return trace.log_prob_sum()\n",
    "    return _log_joint\n",
    "\n",
    "scale_log_joint = make_log_joint(scale)\n",
    "print(scale_log_joint({\"measurement\": 9.5, \"weight\": 8.23}, 8.5))\n",
    "\n",
    "def model():\n",
    "    loc, scale = 0.0, 4.0\n",
    "    t = 0\n",
    "    theta = pyro.param(\"theta\", torch.tensor(2.0))\n",
    "    while t < 100000:\n",
    "        t = t + 1\n",
    "        u = pyro.sample('u_{}'.format(t), pyro.distributions.Uniform(0, 1))\n",
    "        y = pyro.sample('y_{}'.format(t), pyro.distributions.Normal(loc, scale))\n",
    "        if y**2 < u**2 / theta:\n",
    "            # 此时需要条件化 y_t with obs\n",
    "            break            \n",
    "    return t, y\n",
    "\n",
    "def make_log_joint(model):\n",
    "    t, y = model()\n",
    "    name = \"y_{}\".format(t)\n",
    "    print(t, y)\n",
    "    def _log_joint(cond_data, *args, **kwargs):\n",
    "        print(cond_data)\n",
    "        cond_data = {name:v for k, v in cond_data.items()}\n",
    "        print(cond_data)\n",
    "        conditioned_model = poutine.condition(model, data=cond_data)\n",
    "        trace = poutine.trace(conditioned_model).get_trace(*args, **kwargs)\n",
    "        return trace.log_prob_sum()\n",
    "    return _log_joint\n",
    "\n",
    "model_log_joint = make_log_joint(model)\n",
    "print(model_log_joint({\"temp\": torch.tensor(51.5)}))\n",
    "\n",
    "from pyro.poutine.trace_messenger import TraceMessenger\n",
    "from pyro.poutine.condition_messenger import ConditionMessenger \n",
    "\n",
    "def make_log_joint_2(model):\n",
    "    def _log_joint(cond_data, *args, **kwargs):\n",
    "        with TraceMessenger() as tracer:\n",
    "            with ConditionMessenger(data=cond_data):\n",
    "                model(*args, **kwargs)\n",
    "        \n",
    "        trace = tracer.trace\n",
    "        logp = 0.\n",
    "        for name, node in trace.nodes.items():\n",
    "            if node[\"type\"] == \"sample\":\n",
    "                if node[\"is_observed\"]:\n",
    "                    assert node[\"value\"] is cond_data[name]\n",
    "                logp = logp + node[\"fn\"].log_prob(node[\"value\"]).sum()\n",
    "        return logp\n",
    "    return _log_joint\n",
    "\n",
    "scale_log_joint = make_log_joint_2(scale)\n",
    "print(scale_log_joint({\"measurement\": 9.5, \"weight\": 8.23}, 8.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
