{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# 统计计算基础\n",
    "\n",
    "理解概率编程，理解深度概率编程语言 Pyro，我们可以看看统计计算理论基础。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 从拒绝采样谈起\n",
    "\n",
    "国内有名统计教材《高等数理统计》 by 茆诗松关于拒绝采样的证明是错误。正确的证明如下：\n",
    "\n",
    "\n",
    "![reject.png](https://i.loli.net/2020/04/01/WyshxgO3X5moDTA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "从上述的证明可以看到一个随机概率程序 $U, Y -> X$ 可以模拟具备形式 $c \\cdot g(x) \\cdot h(x)$ 的分布该分布代表了一大类分布，我们可以合理的推测：\n",
    "\n",
    "> 可以用 Pyro “随机函数”可以表示任何分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "更有趣的事情是，对于某个密度无显示表示的分布，我们也可以用 Pyro 随机函数抽样。例如，对于某个模型，我们要对其 Bayesian 后验分布抽样，此时我们一般不知道分布的密度函数，但是我们可以得到后验分布的样本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "> **直觉思路**\n",
    "\n",
    "仔细思考拒绝采样以后的本质发现，本质就是从某个分布的 i.i.d 样本序列 $X^{(i)} = \\{X_1^{(i)}, ..., X_t^{(i)}, ....\\}$ 中，按照某个随机动态策略 $T^{(i)}\\sim X^{(i)}$ 选择某些样本 $X_T^{(i)}$。那么我们有一个最简单的思路：\n",
    "\n",
    "- Step 1. 抽取 $N$ 个 i.i.d. 样本 $\\{X_1, ..., X_N\\}$ with density $h(x)$.\n",
    "- Step 2. 计算 weights of each sample $\\{X_1, ..., X_N\\}$，也就是说每个样本的概率 $C \\cdot p(x)$, wherc $C$ is some unknown constant.（Bayesian 后验分布采样就只直到权重，而不知道概率）\n",
    "- Step 3. weighted sampling from  $\\{X_1, ..., X_N\\}$ 将会是一个粗略的估计。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$$\n",
    "P(X_W = x) = \\sum_{x_1, ..., x_N} P(X_W=x|X_1=x_1, ..., X_N=x_N) P(X_1=x_1, ..., X_N=x_N) \\\\\n",
    "\\sum_{x_1, ..., x_N}\\sum_{j=1}^N \\frac{w_j}{w_1 + \\cdots + w_N} \\delta_{x_j}(x)\\cdot h(x_1) \\cdots h(x_N)\n",
    "$$\n",
    "\n",
    "可以看出这种直接权重的方法并不是很好算（我们可以从离散的情况出发来简化思考），这个随机动态策略生成的分布不是我们想要的。那么什么样子的随机动态策略才是我们想要的呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 模型参数的后验分布采样\n",
    "\n",
    "- 为什么 “对于某个模型，我们要对其 Bayesian 后验分布抽样，此时我们一般不知道分布的密度函数，但是我们可以得到后验分布的样本。”？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "这是因为即使没有显示的密度函数，我们依旧可以计算对于数据的后验概率密度  posterior $q_{}(\\theta) = p(\\theta|data) = {p(data | \\theta) p(\\theta) \\over Pr(data)}$ for prior $\\Theta \\sim p_{}(\\theta)$ with unknown constant $c = 1/Pr(data)$. 对应于拒绝采样 $c \\cdot g(x) \\cdot h(x)$\n",
    "\n",
    "$$\n",
    "c \\leftarrow \\frac{1}{p(data)} \\\\\n",
    "g(x) \\leftarrow p(data|\\theta) \\\\\n",
    "h(x) \\leftarrow p(\\theta)\n",
    "$$\n",
    "\n",
    "那么后验分布采样问题就类似拒绝采样问题 (except for $0< g(x) \\leq 1$ for continuous variables, 而如果 data 是离散变量，我们可以直接用拒绝采样)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "> 连续观测变量的模型参数后验分布采样\n",
    "\n",
    "我们用一个简单的变化就饿可以解决连续的问题，注意到 $p(data|\\theta)$ 表示给定每个 $\\theta$ 的值下，有一个密度极大值 $m(\\theta)$，再假定函数 $m(\\theta)$ 是有界函数 with maximum $C$。然后我们后验分布的形式：\n",
    "\n",
    "$$\n",
    "\\frac{C}{p(data)} \\cdot \\frac{p(data|\\theta)}{C} \\cdot p(\\theta)\n",
    "$$\n",
    "\n",
    "这样我们可以在不知道 $p(data)$ 的时候进行拒绝采样，得到该分布的样本了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "总结一下：\n",
    "\n",
    "- 目标是要抽取后验分布的样本。\n",
    "- 我们已知条件是 $p(data|\\theta)p(\\theta)$ 可以直接计算， $p(\\theta)$ 容易抽样，但是 $p(data)$ 计算困难。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 更多采样方法\n",
    "\n",
    "其他常见的采样方法包括 MCMC， MH, HMC 等等。我们先回顾一下 Markov Chain 的相关知识。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Markov Chain\n",
    "\n",
    "我们以问题的形式来回顾相关知识：\n",
    "\n",
    "1）什么样子的 Markov chain with transition density $P(\\cdot|\\cdot)$ 具备某个平稳分布？\n",
    "\n",
    "\n",
    "平稳分布是否存在，以及如果存在是否唯一，这是由过程的特定性质决定的。一个存在 Stationary Distribution 的充分条件是 [马尔科夫链细致平衡条件](https://blog.csdn.net/guolindonggld/article/details/79597491) （里面有代码和数学证明）。简单来说，马尔科夫链要能收敛，需要满足以下条件：\n",
    "\n",
    "    1.可能的状态数是有限的。\n",
    "    2.状态间的转移概率需要固定不变。\n",
    "    3.从任意状态能够转变到任意状态。\n",
    "    4.不能是简单的循环，例如全是从x到y再从y到x。\n",
    "    \n",
    "需要注意的是马尔科夫链是输入为随机变量 $X_0$，输出为无穷多步骤状态转移之后的分布: $p(x) \\rightarrow \\pi(x)$，\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "2）Markov 模型那么多，我怎么把它分类？\n",
    "\n",
    "[Wiki 上面有一个很好的分类](https://www.wikiwand.com/en/Markov_chain#/Special_types_of_Markov_chains), 而我们始终需要明白是这些都是 Pyro 中的随机函数。\n",
    "\n",
    "![markov_model](https://i.loli.net/2020/04/02/eA9Vf2BC7bvNdOS.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### MCMC 采样\n",
    "\n",
    "\n",
    "解决问题思路是类似接受-拒绝采样，那里是以一个常用分布通过一定的接受-拒绝概率得到一个非常见分布，这里是以一个常见的马尔科夫链状态转移矩阵𝑄通过一定的接受-拒绝概率得到目标转移矩阵𝑃。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "在博客的四篇文章介绍得非常清楚了（但是有一些错误），参见：https://www.cnblogs.com/pinard/p/6625739.html\n",
    "\n",
    "    MCMC(一)蒙特卡罗方法\n",
    "    MCMC(二)马尔科夫链\n",
    "    MCMC(三)MCMC采样和M-H采样\n",
    "    MCMC(四)Gibbs采样\n",
    "    \n",
    "这里我们主要从补充数学理解和证明。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "![MH Sampling](https://i.loli.net/2020/04/02/5LD2nhvZrGsWgmq.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "我们有了样本 $x_t$ 之后，想要得到转移概率为 $C * (\\pi(\\cdot) Q(x_t | \\cdot)) * Q(\\cdot | x_t)$ 的一个样本，那么我们此时可以使用拒绝采样，判定是否接受 $Q(\\cdot | x_t)$ 产生的样本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Gibbs 采样：\n",
    "\n",
    "\n",
    "![Gibbs Sampling](https://i.loli.net/2020/04/02/scCShIzv3tPdWU7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "我们需要证明的细致平稳条件是  \n",
    "\n",
    "$$\n",
    "p(x^*|x)  \\pi(x) = P(x|x^*) \\pi(x^*)\n",
    "$$\n",
    "\n",
    "一件非常重要的事情是这些条件概率是可以计算的，联合分布的 Markov factorization 保证了这一点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Conclusion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "本文从经典教程的错误核心拒绝采样定理证明出发，给出正确的证明方法。该证明的一个关键思想是把 i.i.d 样本序列引入进来，体现了通用概率编程(such as Pyro)中“everything thing is stochastic functions” 的思想。数学上来说：For samples $\\bar{X} = \\{X_t\\}_{t\\geq 1}$ and some dynamic control strategy $T(\\bar{X})$, we could simulate any stochastic function with $X_T$. 这样的样本我们称为 Dynamic Control Sample(DCSample)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "具体来说，对于具备形式 $p(x) = c \\cdot g(x) \\cdot h(x)$ 的分布，可以直接进行拒绝采样。如果不能写成或者难以找到这样的形式，我们就更换序列和动态控制策略。从任意的分布 $p_0(x)$ 出发，构建一个满足细致平稳条件的转移概率随机函数 $Q(\\cdot|X_t; p(\\cdot))$，抽取该随机函数的一个 DCSample $X_{t+1}$, 于是我们得到了一个 DCSamples 组成的 Markov 序列（而不是 i.i.d 样本序列），动态控制策略 $T$ 就是任何序列收敛的时刻。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "至此，我们关于抽样的内容基本思路已经清楚，有了样本之后。Monte Carlo 模拟做统计推断的时候，我们可以选择概率测度，从而选择合适的分布进行抽样，以期估计具备更高的收敛速度和效力，这些内容就是"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
